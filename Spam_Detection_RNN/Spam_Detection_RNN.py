# -*- coding: utf-8 -*-
"""Copy of Lab_5_Spam_Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14UpGonXvcxtGWo07sIY7Rp6rzF4fuLi9

# Lab 5: Spam Detection
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np

"""The "SMS Spam Collection Data Set" is available at http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection   """

spam = []
non_spam = []

for line in open('SMSSpamCollection'):
  sep = line.split("\t")
  if sep[0] == "spam":
    sep[0] = 1 #the label for spam is 1
    spam.append(sep)
  else:
    sep[0] = 0 #non-spam messages are labeled 0
    non_spam.append(sep)

#print out a spam SMS and a non-spam SMS
print("Spam: ", spam[0][1])
print("Non-Spam: ", non_spam[0][1])

print(f"There are {len(spam)} spam messages in the dataset")
print(f"There are {len(non_spam)} non-spam messages in the dataset")

############################ splitting dataset ############################

import torchtext
import math

text_field = torchtext.data.Field(sequential=True,      # text sequence
                                  tokenize=lambda x: x, # because are building a character-RNN
                                  include_lengths=True, # to track the length of sequences, for batching
                                  batch_first=True,
                                  use_vocab=True)       # to turn each character into an integer index
label_field = torchtext.data.Field(sequential=False,    # not a sequence
                                   use_vocab=False,     # don't need to track vocabulary
                                   is_target=True,      
                                   batch_first=True,
                                   preprocessing=lambda x: int(x == 'spam')) # convert text to 0 and 1

fields = [('label', label_field), ('sms', text_field)]
dataset = torchtext.data.TabularDataset("SMSSpamCollection", # name of the file
                                        "tsv",               # fields are separated by a tab
                                        fields)
#now, we can have the labels and sms messages for each training example using '.sms' and '.label'

# dataset[0].sms
# dataset[0].label 

num_sms = len(dataset)

train, val, test = dataset.split(split_ratio=[0.6, 0.2, 0.2])

# save the original training examples
old_train_examples = train.examples
# get all the spam messages in `train`
train_spam = []
for item in train.examples:
    if item.label == 1:
        train_spam.append(item)
# duplicate each spam message 6 more times
train.examples = old_train_examples + train_spam * 6

text_field.build_vocab(train)
# text_field.vocab.stoi
# text_field.vocab.itos

#examine batches & padding

train_iter = torchtext.data.BucketIterator(train,
                                           batch_size=32,
                                           sort_key=lambda x: len(x.sms), # to minimize padding
                                           sort_within_batch=True,        # sort within each batch
                                           repeat=False)                  # repeat the iterator for many epochs

num_batch = 0
for batch in train_iter:
    num_batch += 1
    if num_batch > 10:
      break
    msgs, lengths = batch.sms
    max_len = lengths[0].item()
    padded = len(msgs[msgs==1])
    print(f"The max length is {max_len} and the number of <pad> tokens is {padded}")

#obtaining one-hot vectors

ident = torch.eye(10)
print(ident[0]) # one-hot vector
print(ident[1]) # one-hot vector
x = torch.tensor([[1, 2], [3, 4]])
print(ident[x]) # one-hot vectors

############################ create an RNN for spam detection ############################

class SpamDetector(nn.Module):

  def __init__(self, input_size, hidden_size, num_classes):
    super(SpamDetector, self).__init__()
    self.ident = torch.eye(input_size) #identity tensor, 1's on diagonals
    self.hidden_size = hidden_size #number of features that will be output from block
    self.num_classes = num_classes
    
    #start with only 1 rnn layer
    self.rnn = nn.RNN(input_size, hidden_size, 1, batch_first=True)
    self.fc = nn.Linear(hidden_size, num_classes)

  def forward(self, x):
    x = self.ident[x] #convert the input to one-hot
    h0 = torch.zeros(1, x.size(0), self.hidden_size) #initial hidden state
    out, _ = self.rnn(x, h0)
    out = self.fc(out[:, -1, :])
    return out

############################ accuracy function ############################
def get_accuracy(model, data, batch_size=32):
    """ Compute the accuracy of the `model` across a dataset `data`
    
    Example usage:
    
    >>> model = MyRNN() # to be defined
    >>> get_accuracy(model, valid) # the variable `valid` is from above
    """
    data_iter = torchtext.data.BucketIterator(data,
                              batch_size=batch_size,
                              sort_key=lambda x: len(x.sms),
                              sort_within_batch=True,
                              repeat=False)
    corrects = 0
    total_ex = 0

    for batch in data_iter:
      input = batch.sms[0] #isolate the tensor containing messages
      output = model(input)
      preds = output.max(1, keepdim=True)[1]
      corrects += preds.eq(batch.label.view_as(preds)).sum().item()
      total_ex += batch.sms[1].shape[0] #add batch size
    
    return corrects/total_ex

############################ train function ############################

def train_model(model, train_data, val_data, batch_size=32, num_epochs=30, learning_rate=0.0001):
  criterion = nn.CrossEntropyLoss()
  optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)
  train_acc, val_acc, epochs, losses, outputs = [], [], [], [], []
  train_loader = torchtext.data.BucketIterator(train,
                          batch_size=batch_size,
                          sort_key=lambda x: len(x.sms),
                          sort_within_batch=True,
                          repeat=False)


  for epoch in range(num_epochs):
    num_iters = 0
    for batch in train_loader:
      optimizer.zero_grad()
      preds = model(batch.sms[0])
      loss = criterion(preds, batch.label)
      loss.backward()
      optimizer.step()
      num_iters += 1
    
    epochs.append(epoch+1)
    losses.append(float(loss)/num_iters)
    train_accuracy = get_accuracy(model, train_data, batch_size)
    valid_accuracy = get_accuracy(model, val_data, batch_size)
    train_acc.append(train_accuracy)
    val_acc.append(valid_accuracy)
    print("Epoch: {}, Train Acc: {}, Val Acc: {}".format(epoch+1, train_accuracy, valid_accuracy))

  import matplotlib.pyplot as plt
  
  #plotting statistics
  plt.title("Train Loss")
  plt.plot(epochs, losses, label="Train")
  plt.xlabel("Epochs")
  plt.ylabel("Loss")
  plt.show()

  plt.title("Train & Val Accuracies")
  plt.plot(epochs, train_acc, label="Train")
  plt.plot(epochs, val_acc, label="Validation")
  plt.xlabel("Epochs")
  plt.ylabel("Accuracy")
  plt.legend(loc='best')
  plt.show()

############################ training models ############################
first_model = SpamDetector(input_size=len(text_field.vocab), hidden_size=16, num_classes=2)
train_model(first_model, train, val, num_epochs=15)

# keeping number of epochs and learning rate the same
# increasing deep RNN layers from 1 -> 2
# this will add complexity to the model
# increased complexity may allow the model to better recognize patterns in data
# allowing it to make better predictions

#new model
class SpamDetector(nn.Module):

  def __init__(self, input_size, hidden_size, num_classes):
    super(SpamDetector, self).__init__()
    self.ident = torch.eye(input_size) #identity tensor, 1's on diagonals
    self.hidden_size = hidden_size #number of features that will be output from block
    self.num_classes = num_classes
    
    #start with only 1 rnn layer
    self.rnn = nn.RNN(input_size, hidden_size, 2, batch_first=True)
    self.fc = nn.Linear(hidden_size, num_classes)

  def forward(self, x):
    x = self.ident[x] #convert the input to one-hot
    h0 = torch.zeros(2, x.size(0), self.hidden_size) #initial hidden state
    out, _ = self.rnn(x, h0)
    out = self.fc(out[:, -1, :])
    return out

model2 = SpamDetector(input_size=len(text_field.vocab), hidden_size=16, num_classes=2)
train_model(model2, train, val, num_epochs=15)

# Adding an extra RNN layer allowed the model to increase train and val acc
# Keeping this change, I will now modify the model to include max pooling of RNN outputs
# This allows the model to take important information from intermediate RNN outputs
# in order to make a more informed prediction

#introducing max pooling
class SpamDetector(nn.Module):

  def __init__(self, input_size, hidden_size, num_classes):
    super(SpamDetector, self).__init__()
    self.ident = torch.eye(input_size) #identity tensor, 1's on diagonals
    self.hidden_size = hidden_size #number of features that will be output from block
    self.num_classes = num_classes
    
    #start with only 1 rnn layer
    self.rnn = nn.RNN(input_size, hidden_size, 2, batch_first=True)
    self.fc = nn.Linear(hidden_size, num_classes)

  def forward(self, x):
    x = self.ident[x] #convert the input to one-hot
    h0 = torch.zeros(2, x.size(0), self.hidden_size) #initial hidden state
    out, _ = self.rnn(x, h0)
    out = self.fc(torch.max(out, dim=1)[0]) #max pooling
    return out

model3 = SpamDetector(input_size=len(text_field.vocab), hidden_size=16, num_classes=2)
train_model(model3, train, val, num_epochs=15)

# Evidently, the above changes increased train and val acc
# Keeping these changes, I will increase learning rate
# This will allow the optimizer to take larger steps towards the minimum of the cost function
# I will change the learning rate to 0.01 and observe the results

model4 = SpamDetector(input_size=len(text_field.vocab), hidden_size=16, num_classes=2)
train_model(model4, train, val, num_epochs=15, learning_rate = 0.01)

# The loss and accuracy plots appear to be much more turbulent
# However, val accuracy reaches up to 98 % at some one point
# I will decrease the learning rate to 0.001 for less fluctuating results

model5 = SpamDetector(input_size=len(text_field.vocab), hidden_size=16, num_classes=2)
train_model(model5, train, val, num_epochs=15, learning_rate = 0.001)

# The loss/accuracy is still fluctuating quite a bit
# but the overall val accuracy is improving
# I will increase batch size and learning rate
# This enables the optimizer to take larger steps to reduce cost
# There will also be more examples that losses will be averaged over 
# This reduces the effect of outliers and allows the model to make more "informed" updates

#batch size: 64, learning rate: 0.008
model6 = SpamDetector(input_size=len(text_field.vocab), hidden_size=16, num_classes=2)
train_model(model6, train, val, batch_size=64, num_epochs=15, learning_rate = 0.008)

############################ testing performance on model subsets ############################

# Create a Dataset of only spam validation examples
# spam messages are "positive labels"
valid_spam = torchtext.data.Dataset(
    [e for e in val.examples if e.label == 1],
    val.fields)

fn_rate = 1 - get_accuracy(model6, valid_spam)

# Create a Dataset of only non-spam validation examples
valid_nospam = torchtext.data.Dataset(
    [e for e in val.examples if e.label == 0],
    val.fields)

fp_rate = 1 - get_accuracy(model6, valid_nospam)

print("False positive rate: {}, False negative rate: {}".format(fp_rate, fn_rate))

############################ testing model ############################

test_acc = get_accuracy(model6, test)
print(test_acc)

#test data containing only spam messages
test_spam = torchtext.data.Dataset(
    [e for e in test.examples if e.label == 1],
    test.fields)

fn_rate = 1 - get_accuracy(model6, test_spam)

#test data containing only non-spam messages
test_nospam = torchtext.data.Dataset(
    [e for e in test.examples if e.label == 0],
    test.fields)

fp_rate = 1 - get_accuracy(model6, test_nospam)

print("False positive rate: {}, False negative rate: {}".format(fp_rate, fn_rate))

############################ testing model on a generated SMS msg ############################

msg = "machine learning is sooo cool!"

char_idxs = []

for char in msg:
  char_idxs += [text_field.vocab.stoi[char]]

input = torch.from_numpy(np.array([char_idxs]))
pred = model6(input)
softmax = nn.Softmax()
print(softmax(pred))
pred = torch.argmax(pred).item()


if pred == 0:
  print("The model predicts the message is not spam")
else:
  print("the model predicts the message is spam")